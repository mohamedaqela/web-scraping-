{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55fb141e-2c3d-4f7d-960d-d320b610d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data has been extracted and saved to the file: Extract_Text_Data.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract Text Data:\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import csv \n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    headings = soup.find_all(['h1','h2'])\n",
    "   \n",
    "    paragraphs = soup.find_all('p')\n",
    "    list_items = soup.find_all('li')\n",
    "    \n",
    "\n",
    "    text_data = []\n",
    "    for tag in (headings + paragraphs + list_items):\n",
    "        text_data.append({\n",
    "            \"type\": tag.name, \n",
    "            \"content\": tag.get_text(strip=True) \n",
    "        })\n",
    "\n",
    "    with open(\"Extract_Text_Data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=[\"type\", \"content\"])\n",
    "        writer.writeheader() \n",
    "        writer.writerows(text_data) \n",
    "\n",
    "    print(\"Text data has been extracted and saved to the file: Extract_Text_Data.csv\")\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adad00a-55fb-45c6-9959-8033e883003b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65fc3161-843d-488d-88a8-b9e625c4f4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table data has been extracted and saved to the file: Extract_Table_Data.csv\n"
     ]
    }
   ],
   "source": [
    "#2. Extract Table Data:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import csv \n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    table = soup.find(\"table\") \n",
    "\n",
    "    if table:\n",
    "        headers = [header.get_text(strip=True) for header in table.find_all(\"th\")]\n",
    " \n",
    "        rows = []\n",
    "        for row in table.find_all(\"tr\")[1:]: \n",
    "            columns = [col.get_text(strip=True) for col in row.find_all(\"td\")]\n",
    "            rows.append(dict(zip(headers, columns)))\n",
    "    \n",
    "        with open(\"Extract_Table_Data.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
    "            writer.writeheader() \n",
    "            writer.writerows(rows) \n",
    "\n",
    "        print(\"Table data has been extracted and saved to the file: Extract_Table_Data.csv\")\n",
    "    else:\n",
    "        print(\"No table found in the page.\")\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec0cfb-ad36-40b3-871e-587ce23d777b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f3b36d2e-7bc5-4b56-a64f-51890b086a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been extracted and saved to the file: Product_Information.json\n"
     ]
    }
   ],
   "source": [
    "#3. Extract Product Information (Cards Section):\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    cards = soup.find_all(class_=\"book-card\") \n",
    "    \n",
    "    product_info = []\n",
    "    \n",
    "    for card in cards:\n",
    "        title = card.find(class_=\"title\").get_text(strip=True) if card.find(class_=\"title\") else \"N/A\"\n",
    "        price = card.find(class_=\"price\").get_text(strip=True) if card.find(class_=\"price\") else \"N/A\"\n",
    "        stock_status = card.find(class_=\"stock-status\").get_text(strip=True) if card.find(class_=\"stock-status\") else \"N/A\"\n",
    "        button_text = card.find(\"button\").get_text(strip=True) if card.find(\"button\") else \"N/A\"\n",
    "        \n",
    "        product_info.append({\n",
    "            \"Book Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"Stock Availability\": stock_status,\n",
    "            \"Button Text\": button_text\n",
    "        })\n",
    "    \n",
    "    with open(\"Product_Information.json\", mode=\"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(product_info, json_file, indent=4)\n",
    "\n",
    "    print(\"The data has been extracted and saved to the file: Product_Information.json\")\n",
    "else:\n",
    "    print(\"Failed to load the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ea256-3431-4025-a6c6-fbb3f649544d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18d64638-69c3-4e7e-a5dc-8bc991dd1b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form details have been extracted and saved to the file: Form_Details.json\n"
     ]
    }
   ],
   "source": [
    "#4. Extract Form Details:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    forms = soup.find_all(\"form\")\n",
    "\n",
    "    form_details = [{\n",
    "        \"Field Name\": input_tag.get(\"name\", \"N/A\"),\n",
    "        \"Input Type\": input_tag.get(\"type\", \"N/A\"),\n",
    "        \"Default Value\": input_tag.get(\"value\", \"N/A\")\n",
    "    } for form in forms for input_tag in form.find_all(\"input\")]\n",
    "    \n",
    "    with open(\"Form_Details.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(form_details, json_file, indent=4)\n",
    "\n",
    "    print(\"Form details have been extracted and saved to the file: Form_Details.json\")\n",
    "else:\n",
    "    print(\"Failed to load the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092a649-b4a0-4689-9c69-670314738f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e59bbbe6-23f6-4dd6-8d3b-3f00c64c0979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links and multimedia content have been extracted and saved to the file: Links_and_Multimedia.json\n"
     ]
    }
   ],
   "source": [
    "#5. Extract Links and Multimedia:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    links = [{\"link_text\": a.get_text(strip=True), \"href\": a.get(\"href\", \"N/A\")} for a in soup.find_all(\"a\")]\n",
    "    \n",
    "    iframes = [{\"video_url\": iframe.get(\"src\", \"N/A\")} for iframe in soup.find_all(\"iframe\")]\n",
    "\n",
    "    data = {\"links\": links, \"videos\": iframes}\n",
    "    \n",
    "    with open(\"Links_and_Multimedia.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "    print(\"Links and multimedia content have been extracted and saved to the file: Links_and_Multimedia.json\")\n",
    "else:\n",
    "    print(\"Failed to load the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e491ca5-e064-491f-a16c-86c385f2cb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "228d1429-b959-4a19-a138-a3fd58537922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been successfully extracted and saved to 'Featured_Products.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    products = soup.select(\".featured-products .product-card\")\n",
    "    \n",
    "    featured_products = []\n",
    "    for product in products:\n",
    "        product_id = product.get(\"data-id\", \"N/A\")  \n",
    "        product_name = product.find(\"span\", class_=\"name\").get_text(strip=True, default=\"N/A\")  \n",
    "        product_price = product.find(\"span\", class_=\"price\", style=\"display: none;\").get_text(strip=True, default=\"N/A\")  \n",
    "        product_colors = product.find(\"span\", class_=\"colors\").get_text(strip=True, default=\"N/A\") \n",
    "        \n",
    "        featured_products.append({\n",
    "            \"id\": product_id,\n",
    "            \"name\": product_name,\n",
    "            \"price\": product_price,\n",
    "            \"colors\": product_colors\n",
    "        })\n",
    "    \n",
    "    with open(\"Featured_Products.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(featured_products, json_file, indent=4)\n",
    "\n",
    "    print(\"Data has been successfully extracted and saved to 'Featured_Products.json'.\")\n",
    "else:\n",
    "    print(\"Failed to load the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283048d-b6c7-4761-90c7-d29f66305d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800d00b9-b1cb-4fe1-8926-214f47226e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
